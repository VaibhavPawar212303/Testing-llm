import os
import requests
import json
import re
import google.generativeai as genai
from deepeval.models.base_model import DeepEvalBaseLLM
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

# ANSI Colors for Terminal Output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'


class BaseReportingModel(DeepEvalBaseLLM):
    """Base class with common reporting functionality."""
    
    def __init__(self):
        self.last_prompt = ""
        self.last_response = ""
        self.last_metrics = {}
        self.last_thought_process = ""
        self.generation_start_time = None
        self.generation_end_time = None
    
    def get_report_data(self) -> Dict[str, Any]:
        """Returns the current state of the model run for combined reporting."""
        return {
            "model": getattr(self, 'model_name', 'unknown'),
            "duration_seconds": (
                (self.generation_end_time - self.generation_start_time).total_seconds()
                if self.generation_start_time and self.generation_end_time else 0
            ),
            "prompt": self.last_prompt,
            "response": self.last_response,
            "metrics": self.last_metrics,
            "thought_process": self.last_thought_process
        }

    def save_report(self, report_dir: str, prefix: str = "llm_analysis"):
        """Universal report saving method."""
        try:
            abs_report_dir = os.path.abspath(report_dir)
            Path(abs_report_dir).mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = os.path.join(abs_report_dir, f"{prefix}_{timestamp}.json")
            markdown_file = os.path.join(abs_report_dir, f"{prefix}_{timestamp}.md")
            
            report_data = {
                "timestamp": timestamp,
                "model": getattr(self, 'model_name', 'unknown'),
                "endpoint": self._get_endpoint(),
                "generation_time": {
                    "start": self.generation_start_time.isoformat() if self.generation_start_time else None,
                    "end": self.generation_end_time.isoformat() if self.generation_end_time else None,
                    "duration_seconds": (
                        (self.generation_end_time - self.generation_start_time).total_seconds()
                        if self.generation_start_time and self.generation_end_time else None
                    )
                },
                "prompt": self.last_prompt,
                "response": self.last_response,
                "metrics": self.last_metrics
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self._save_markdown_report(markdown_file, report_data)
            print(f"\n{Colors.OKGREEN}üìÑ Report saved: {report_file}{Colors.ENDC}")
            return report_file
            
        except Exception as e:
            print(f"{Colors.WARNING}‚ö†Ô∏è  Report save failed: {e}{Colors.ENDC}")
            return None
    
    def _get_endpoint(self) -> str:
        return "N/A"
    
    def _save_markdown_report(self, filepath: str, data: Dict[str, Any]):
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# LLM Analysis Report\n\n")
            f.write(f"**Model:** {data['model']}\n")
            f.write(f"**Timestamp:** {data['timestamp']}\n")
            f.write(f"**Endpoint:** {data['endpoint']}\n\n")
            if data['generation_time']['duration_seconds']:
                f.write(f"## ‚è±Ô∏è Generation Time: {data['generation_time']['duration_seconds']:.2f}s\n\n")
            if data['metrics']:
                f.write(f"## üíª Hardware Metrics\n\n")
                self._write_metrics_markdown(f, data['metrics'])
            f.write(f"## üìù Prompt\n\n```\n{data['prompt']}\n```\n\n")
            f.write(f"## üìä Response\n\n{data['response']}\n\n")
            f.write(f"---\n*Generated by LLM Testing Framework*\n")
    
    def _write_metrics_markdown(self, f, metrics: Dict[str, Any]):
        if 'duration_ms' in metrics:
            f.write(f"- Duration: {metrics['duration_ms']/1000:.2f}s\n")
        if 'cpu' in metrics:
            cpu = metrics['cpu']
            f.write(f"- CPU Avg: {cpu.get('avg_usage_percent', 'N/A')}% | Peak: {cpu.get('peak_usage_percent', 'N/A')}%\n")
        if 'ram' in metrics:
            ram = metrics['ram']
            f.write(f"- RAM Avg: {ram.get('avg_used_gb', 'N/A')}GB | Peak: {ram.get('peak_used_gb', 'N/A')}GB\n")
        f.write("\n")


class OllamaModel(BaseReportingModel):
    def __init__(self, model_name: str, base_url: str):
        super().__init__()
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.api_key = os.getenv("SUT_API_KEY")

    def load_model(self): return self.model_name
    def _get_endpoint(self) -> str: return f"{self.base_url}/playwright"

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt
        self.generation_start_time = datetime.now()
        url = f"{self.base_url}/playwright"
        headers = {"x-api-key": self.api_key, "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}], "stream": False}
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=600)
            self.generation_end_time = datetime.now()
            if response.status_code != 200: 
                print(f"{Colors.FAIL}‚ùå Ollama Error {response.status_code}: {response.text}{Colors.ENDC}")
                return f"Ollama Error: {response.status_code}"
            
            res_json = response.json()
            
            # DEBUG: Print raw response
            print(f"\n{Colors.OKCYAN}üîç DEBUG - Ollama Raw Response:{Colors.ENDC}")
            print(json.dumps(res_json, indent=2)[:500])  # First 500 chars
            
            if 'metrics' in res_json:
                self.last_metrics = res_json['metrics']
                self._display_metrics()
            
            content = self._extract_content(res_json)
            if content:
                self.last_response = content
                print(f"\n{Colors.OKGREEN}‚úÖ Ollama Response Length: {len(content)} chars{Colors.ENDC}")
                self.save_report("./data/results/ollama_reports", "ollama")
                return content
            
            print(f"{Colors.WARNING}‚ö†Ô∏è No content in Ollama response{Colors.ENDC}")
            return "No content in response"
            
        except Exception as e:
            self.generation_end_time = datetime.now()
            print(f"{Colors.FAIL}‚ùå Ollama Exception: {str(e)}{Colors.ENDC}")
            return f"Ollama Error: {str(e)}"
    
    def _extract_content(self, response: Dict[str, Any]) -> Optional[str]:
        if not isinstance(response, dict): return str(response)
        for key in ['analysis', 'response', 'content', 'text', 'output']:
            if key in response and response[key]: return response[key]
        if 'message' in response:
            msg = response['message']
            if isinstance(msg, dict) and 'content' in msg: return msg['content']
        return None
    
    def _display_metrics(self):
        if not self.last_metrics: return
        print(f"\n{Colors.BOLD}--- Hardware Metrics ---{Colors.ENDC}")
        if 'duration_ms' in self.last_metrics: print(f"‚è±Ô∏è  Duration: {self.last_metrics['duration_ms']/1000:.2f}s")
        print(f"{Colors.BOLD}------------------------{Colors.ENDC}\n")

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"Ollama: {self.model_name}"


class RobustGeminiModel(BaseReportingModel):
    def __init__(self, model_name: str):
        super().__init__()
        self.model_name = model_name
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        
        # üî• NEW: Store captured data for DeepEval metrics
        self.captured_statements = []
        self.captured_verdicts = []
        
        # üî• FIX: Add max_output_tokens and stronger repetition penalty
        self.generation_config = {
            "temperature": 0.1,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 2048,  # Prevent infinite loops
        }
        
        # üî• FIX: Add safety settings to prevent blocking
        self.safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
        ]
        
        self.model = genai.GenerativeModel(
            model_name=model_name, 
            generation_config=self.generation_config,
            safety_settings=self.safety_settings
        )

    def load_model(self): return self.model
    def _get_endpoint(self) -> str: return f"Gemini API: {self.model_name}"

    def _sanitize_json_response(self, text: str) -> str:
        """Enhanced JSON sanitization with repetition detection and quote fixing"""
        print(f"\n{Colors.OKCYAN}üîç DEBUG - Raw Gemini Response (first 1000 chars):{Colors.ENDC}")
        print(text[:1000])
        
        # üî• FIX: Detect infinite repetition
        if self._detect_repetition(text):
            print(f"{Colors.WARNING}‚ö†Ô∏è DETECTED INFINITE REPETITION - Truncating{Colors.ENDC}")
            text = self._truncate_repetition(text)
        
        # Remove markdown fences
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        
        # Extract JSON object
        start = text.find('{')
        end = text.rfind('}')
        if start != -1 and end != -1:
            text = text[start:end+1]
        
        # üî• CRITICAL FIX: Replace problematic patterns in JSON strings
        # Fix HTML/code attributes that break JSON: role="button" -> role='button'
        text = re.sub(r'(\w+)="([^"]*)"', r"\1='\2'", text)
        text = re.sub(r'(\w+)=\'([^\']*)\'', r"\1='\2'", text)
        
        # Fix backticks in JSON strings (convert to single quotes)
        text = text.replace('`', "'")
        
        # üî• NEW FIX: Gemini sometimes uses wrong key name
        # If prompt expects "verdicts" but response has "statements", fix it
        if '"verdict":' in text and '"statements":' in text and '"verdicts":' not in text:
            print(f"{Colors.WARNING}‚ö†Ô∏è Fixing key: 'statements' -> 'verdicts'{Colors.ENDC}")
            # Replace the outer "statements" key with "verdicts"
            text = re.sub(r'"statements"\s*:', '"verdicts":', text, count=1)
        
        print(f"\n{Colors.OKGREEN}‚úÖ Sanitized JSON (first 500 chars):{Colors.ENDC}")
        print(text[:500])
        
        return text.strip()
    
    def _detect_repetition(self, text: str, threshold: int = 10) -> bool:
        """Detect if a phrase repeats more than threshold times"""
        # Look for patterns like '"The locator name should be \'roleContains\'."'
        lines = text.split('\n')
        if len(lines) < 10:
            return False
        
        # Check if same line repeats
        from collections import Counter
        line_counts = Counter(lines)
        max_count = max(line_counts.values()) if line_counts else 0
        return max_count > threshold
    
    def _truncate_repetition(self, text: str) -> str:
        """Keep only unique statements from the beginning"""
        lines = text.split('\n')
        seen = set()
        unique_lines = []
        
        for line in lines:
            if line.strip() and line.strip() not in seen:
                unique_lines.append(line)
                seen.add(line.strip())
            
            # Stop after 20 unique lines
            if len(unique_lines) >= 20:
                break
        
        return '\n'.join(unique_lines)
    
    def _fix_reason_response(self, text: str, prompt: str) -> str:
        """Fix responses where DeepEval expects 'reason' but gets 'statements'"""
        try:
            # Check if this is a reason generation request
            is_reason_request = 'reason for the score' in prompt.lower() or 'concise reason' in prompt.lower()
            
            if is_reason_request:
                parsed = json.loads(text)
                
                # If response has 'statements' but prompt expects 'reason'
                if 'statements' in parsed and 'reason' not in parsed:
                    print(f"{Colors.WARNING}‚ö†Ô∏è Converting 'statements' array to 'reason' string{Colors.ENDC}")
                    
                    # Combine statements into a single reason string
                    statements = parsed.get('statements', [])
                    combined_reason = ' '.join(statements) if statements else "Score reflects answer quality."
                    
                    return json.dumps({"reason": combined_reason}, indent=2)
            
            return text
            
        except json.JSONDecodeError:
            return text
        except Exception as e:
            print(f"{Colors.WARNING}‚ö†Ô∏è Reason fix failed: {e}{Colors.ENDC}")
            return text
    
    def _capture_response_data(self, parsed: dict):
        """Capture statements and verdicts from parsed JSON for later retrieval"""
        if 'statements' in parsed:
            self.captured_statements = parsed['statements']
            print(f"{Colors.OKCYAN}üì¶ Captured {len(self.captured_statements)} statements{Colors.ENDC}")
        
        if 'verdicts' in parsed:
            self.captured_verdicts = parsed['verdicts']
            print(f"{Colors.OKCYAN}üì¶ Captured {len(self.captured_verdicts)} verdicts{Colors.ENDC}")
    
    def _manual_statement_extraction(self, text: str) -> str:
        """Fallback: Extract statements manually when JSON parsing fails"""
        try:
            # Find all lines that look like statements (quoted strings)
            statement_pattern = r'"([^"]{10,200})"'  # Strings between 10-200 chars
            matches = re.findall(statement_pattern, text)
            
            # Filter out keys and keep only values that look like statements
            statements = [
                m for m in matches 
                if not m.endswith(':') 
                and not m in ['statements', 'statement', 'text', 'verdict', 'verdicts', 'reason']
                and len(m.split()) > 3  # At least 4 words
            ]
            
            # Remove duplicates while preserving order
            seen = set()
            unique_statements = []
            for s in statements:
                if s not in seen and len(unique_statements) < 10:
                    unique_statements.append(s)
                    seen.add(s)
            
            # üî• FIX: Detect if this is a verdicts response (has "verdict" key)
            is_verdicts_response = '"verdict"' in text or "'verdict'" in text
            
            if is_verdicts_response:
                # Extract verdicts format
                result = self._extract_verdicts_manually(text)
            else:
                # Build standard statements JSON
                result = {
                    "statements": unique_statements
                }
            
            print(f"{Colors.OKGREEN}‚úÖ Manually extracted {len(unique_statements)} items{Colors.ENDC}")
            return json.dumps(result, indent=4)
            
        except Exception as e:
            print(f"{Colors.FAIL}‚ùå Manual extraction failed: {e}{Colors.ENDC}")
            # Return appropriate empty structure
            if '"verdict"' in text:
                return '{"verdicts": []}'
            return '{"statements": []}'
    
    def _extract_verdicts_manually(self, text: str) -> dict:
        """Extract verdicts from malformed JSON"""
        try:
            # Find all verdict patterns
            verdict_pattern = r'"verdict":\s*"(yes|no|idk)"'
            reason_pattern = r'"reason":\s*"([^"]+)"'
            
            verdicts_matches = re.findall(verdict_pattern, text, re.IGNORECASE)
            reasons_matches = re.findall(reason_pattern, text)
            
            # Pair them up
            verdicts = []
            for i, verdict in enumerate(verdicts_matches):
                reason = reasons_matches[i] if i < len(reasons_matches) else "N/A"
                verdicts.append({
                    "verdict": verdict.lower(),
                    "reason": reason
                })
            
            # üî• CRITICAL: Use "verdicts" key, not "statements"
            return {"verdicts": verdicts}
            
        except Exception as e:
            print(f"{Colors.WARNING}‚ö†Ô∏è Verdict extraction failed: {e}{Colors.ENDC}")
            return {"verdicts": []}

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt
        self.generation_start_time = datetime.now()
        enhanced_prompt = self._enhance_prompt_for_json(prompt)
        
        print(f"\n{Colors.OKCYAN}üîç DEBUG - Gemini Prompt:{Colors.ENDC}")
        print(enhanced_prompt[:300])
        
        try:
            response = self.model.generate_content(enhanced_prompt)
            self.generation_end_time = datetime.now()
            
            # üî• FIX: Check if response was blocked
            if not response.text:
                print(f"{Colors.FAIL}‚ùå Gemini response blocked or empty{Colors.ENDC}")
                print(f"Candidates: {response.candidates}")
                return "Error: Response blocked by safety filters"
            
            clean_text = self._sanitize_json_response(response.text)
            
            # üî• NEW: Fix reason/statements mismatch BEFORE validation
            clean_text = self._fix_reason_response(clean_text, prompt)
            
            # üî• FIX: Validate JSON before returning
            try:
                parsed = json.loads(clean_text)
                key = 'reason' if 'reason' in parsed else 'statements' if 'statements' in parsed else 'verdicts'
                count = len(parsed.get(key, [])) if isinstance(parsed.get(key), list) else 1
                print(f"{Colors.OKGREEN}‚úÖ Valid JSON generated with {count} {key}{Colors.ENDC}")
                
                # üî• NEW: CAPTURE statements and verdicts for later retrieval
                self._capture_response_data(parsed)
                
            except json.JSONDecodeError as e:
                print(f"{Colors.FAIL}‚ùå Invalid JSON: {e}{Colors.ENDC}")
                print(f"Problematic JSON: {clean_text[:500]}")
                
                # üî• FALLBACK: Try to manually extract statements from broken JSON
                print(f"{Colors.WARNING}‚ö†Ô∏è Attempting manual statement extraction...{Colors.ENDC}")
                clean_text = self._manual_statement_extraction(response.text)
                
                # Try to capture from fallback too
                try:
                    parsed = json.loads(clean_text)
                    self._capture_response_data(parsed)
                except:
                    pass
            
            self.last_response = clean_text 
            self.save_report("./data/results/gemini_reports", "gemini")
            return clean_text 
            
        except Exception as e:
            self.generation_end_time = datetime.now()
            print(f"{Colors.FAIL}‚ùå Gemini Exception: {str(e)}{Colors.ENDC}")
            return f"Gemini Error: {str(e)}"
    
    def _enhance_prompt_for_json(self, prompt: str) -> str:
        """Enhanced prompt to prevent repetition and code snippets"""
        if "json" in prompt.lower() or "{" in prompt:
            # üî• FIX: Detect response type
            expects_verdicts = '"verdicts"' in prompt or 'verdict' in prompt.lower()
            expects_reason = 'reason for the score' in prompt.lower() or 'concise reason' in prompt.lower()
            
            if expects_reason:
                example = '''{
    "reason": "The answer is partially relevant because it addresses the main topic but misses key details."
}'''
                key_instruction = "Return JSON with single 'reason' key containing a string"
            elif expects_verdicts:
                example = '''{
    "verdicts": [
        {"verdict": "yes", "reason": "Relevant to the input"},
        {"verdict": "no", "reason": "Not related"},
        {"verdict": "idk", "reason": "Unclear"}
    ]
}'''
                key_instruction = "Return JSON with 'verdicts' key (NOT 'statements')"
            else:
                example = '''{
    "statements": [
        "Statement 1 about X",
        "Statement 2 about Y",
        "Statement 3 about Z"
    ]
}'''
                key_instruction = "Return JSON with 'statements' key"
            
            # üî• FIX: Add explicit instructions to prevent repetition AND code snippets
            return f"""{prompt}

**CRITICAL INSTRUCTIONS:**
1. {key_instruction}
2. Return ONLY valid JSON - no markdown, no explanations
3. Each item should be UNIQUE - DO NOT repeat
4. Limit to maximum 10 unique items
5. DO NOT include code snippets, HTML tags, or examples in text
6. Keep text as plain descriptions only
7. Use single quotes (') instead of double quotes (") inside text values
8. If you find yourself repeating, STOP immediately

Example of CORRECT output:
{example}

Example of INCORRECT output (DO NOT DO THIS):
{{
    "statements": [  // ‚ùå Wrong key if expecting verdicts or reason
        "Same statement repeated again",
        "Same statement repeated again"
    ]
}}"""
        return prompt

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"Gemini ({self.model_name})"


class MultiAgentSUT(BaseReportingModel):
    def __init__(self, model_name: str, base_url: str, endpoint: str, api_key: str):
        super().__init__()
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.endpoint = endpoint
        self.api_key = api_key
        self.last_thought_process = ""

    def load_model(self): return self.base_url
    def _get_endpoint(self) -> str: return f"{self.base_url}{self.endpoint}"

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt
        self.last_thought_process = ""
        self.generation_start_time = datetime.now()
        url = f"{self.base_url}{self.endpoint}"
        headers = {"x-api-key": self.api_key, "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"prompt": prompt, "model": self.model_name, "stream": True}
        full_analysis = ""
        
        try:
            response = requests.post(url, json=payload, headers=headers, stream=True, timeout=600)
            if response.status_code != 200: 
                print(f"{Colors.FAIL}‚ùå MultiAgent Error {response.status_code}: {response.text}{Colors.ENDC}")
                return f"MultiAgent Error: {response.status_code}"
            
            print(f"\n{Colors.OKCYAN}ü§ñ {self.model_name} generating...{Colors.ENDC}")
            
            for line in response.iter_lines():
                if line:
                    try:
                        decoded_line = line.decode('utf-8')
                        if decoded_line.startswith("data: "):
                            json_str = decoded_line.replace("data: ", "").strip()
                            if not json_str or json_str == "[DONE]": continue
                            
                            data = json.loads(json_str)
                            
                            if data.get('done'):
                                if 'metrics' in data: 
                                    self.last_metrics = data['metrics']
                                break
                            
                            dtype = data.get('type', 'analysis')
                            chunk = data.get('chunk') or data.get('content') or ''
                            
                            if chunk:
                                if dtype == 'thought':
                                    self.last_thought_process += chunk
                                    print(f"{Colors.DIM}{chunk}{Colors.ENDC}", end="", flush=True)
                                else:
                                    full_analysis += chunk
                                    print(chunk, end="", flush=True)
                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        print(f"\n{Colors.WARNING}‚ö†Ô∏è Stream parse error: {e}{Colors.ENDC}")
                        continue
            
            self.generation_end_time = datetime.now()
            self.last_response = full_analysis
            
            print(f"\n{Colors.OKGREEN}‚úÖ Complete (Length: {len(full_analysis)} chars){Colors.ENDC}\n")
            
            if self.last_metrics: 
                self._display_metrics()
            
            self.save_report("./data/results/multiagent_reports", "multiagent")
            return full_analysis if full_analysis else "Error: Empty response"
            
        except Exception as e:
            self.generation_end_time = datetime.now()
            print(f"{Colors.FAIL}‚ùå MultiAgent Exception: {str(e)}{Colors.ENDC}")
            return f"MultiAgent Error: {str(e)}"
    
    def _display_metrics(self):
        if not self.last_metrics: return
        print(f"{Colors.BOLD}--- Performance ---{Colors.ENDC}")
        if 'duration_ms' in self.last_metrics: 
            print(f"‚è±Ô∏è  {self.last_metrics['duration_ms']/1000:.2f}s")
        print(f"{Colors.BOLD}-------------------{Colors.ENDC}\n")

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"MultiAgent: {self.model_name}"


# Compatibility Alias
GeminiModel = RobustGeminiModel


# ==========================================
# üèÜ ENHANCED BUILD REPORTER WITH DEBUG
# ==========================================
def generate_build_result(sut_instance, judge_instance, metrics: List[Any], output_dir: str = "./data/results/builds"):
    """
    AGGREGATOR: Combines SUT, Judge, and DeepEval Pipeline with enhanced debugging.
    """
    print(f"\n{Colors.OKBLUE}üìä AGGREGATING PIPELINE DATA...{Colors.ENDC}")
    
    try:
        abs_output_dir = os.path.abspath(output_dir)
        Path(abs_output_dir).mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Collect SUT and Judge Data
        sut_data = sut_instance.get_report_data() if hasattr(sut_instance, 'get_report_data') else {}
        judge_data = judge_instance.get_report_data() if hasattr(judge_instance, 'get_report_data') else {}
        
        print(f"\n{Colors.OKCYAN}üîç DEBUG - Judge Response Length: {len(judge_data.get('response', ''))}{Colors.ENDC}")
        
        # üî• NEW: Retrieve captured data from judge instance
        captured_statements = getattr(judge_instance, 'captured_statements', [])
        captured_verdicts = getattr(judge_instance, 'captured_verdicts', [])
        
        print(f"{Colors.OKCYAN}üîç DEBUG - Judge Captured Data:{Colors.ENDC}")
        print(f"  üìù Captured Statements: {len(captured_statements)}")
        print(f"  ‚öñÔ∏è  Captured Verdicts: {len(captured_verdicts)}")
        
        # 2. Collect DeepEval Analytics with ENHANCED DEBUGGING
        eval_results = []
        overall_passed = True
        
        for i, m in enumerate(metrics):
            print(f"\n{Colors.OKCYAN}üîç DEBUG - Metric #{i}: {m.__class__.__name__}{Colors.ENDC}")
            
            # üî• CRITICAL FIX: Get DeepEval's actual calculated score
            # DeepEval stores the score after evaluation completes
            actual_score = getattr(m, 'score', None)
            threshold = getattr(m, 'threshold', 0)
            reason = getattr(m, 'reason', None)
            success = getattr(m, 'success', None)
            
            # üî• If score is None, DeepEval hasn't finished calculating yet
            if actual_score is None:
                print(f"  {Colors.WARNING}‚ö†Ô∏è Score not yet calculated by DeepEval{Colors.ENDC}")
                # üî• FALLBACK: Calculate from verdicts only as temporary measure
                if captured_verdicts:
                    yes_count = sum(1 for v in captured_verdicts if v.get('verdict') == 'yes')
                    idk_count = sum(1 for v in captured_verdicts if v.get('verdict') == 'idk')
                    total_count = len(captured_verdicts)
                    if total_count > 0:
                        # AnswerRelevancy: (yes + idk) / total (idk is not penalized)
                        actual_score = (yes_count + idk_count) / total_count
                        print(f"  {Colors.WARNING}‚ö†Ô∏è Using fallback calculation: ({yes_count} yes + {idk_count} idk) / {total_count} = {actual_score:.2f}{Colors.ENDC}")
                else:
                    actual_score = 0.0
            else:
                print(f"  {Colors.OKGREEN}‚úÖ Using DeepEval's calculated score: {actual_score:.2f}{Colors.ENDC}")
            
            actual_score = float(actual_score) if actual_score is not None else 0.0
            
            # üî• FIX: Try metric attributes first, then fall back to captured data
            statements = getattr(m, 'statements', None)
            if statements is None or (isinstance(statements, list) and len(statements) == 0):
                statements = captured_statements
                print(f"  üìù Using captured statements: {len(statements)}")
            else:
                print(f"  üìù Statements from metric: {len(statements)}")
            
            if statements and len(statements) > 0:
                print(f"  First statement: {statements[0][:100]}")

            # üî• FIX: Try metric attributes first, then fall back to captured data
            raw_verdicts = getattr(m, 'verdicts', None)
            if raw_verdicts is None or (isinstance(raw_verdicts, list) and len(raw_verdicts) == 0):
                # Use captured verdicts
                verdicts = []
                for v in captured_verdicts:
                    if isinstance(v, dict):
                        verdicts.append({
                            "verdict": v.get('verdict', 'N/A'),
                            "reason": v.get('reason', 'N/A')
                        })
                print(f"  ‚öñÔ∏è  Using captured verdicts: {len(verdicts)}")
            else:
                # Use metric verdicts
                verdicts = []
                for v in raw_verdicts:
                    verdict_data = {
                        "verdict": getattr(v, 'verdict', 'N/A'),
                        "reason": getattr(v, 'reason', 'N/A')
                    }
                    verdicts.append(verdict_data)
                print(f"  ‚öñÔ∏è  Verdicts from metric: {len(verdicts)}")
            
            if verdicts and len(verdicts) > 0:
                for v in verdicts[:3]:  # Show first 3
                    print(f"    - {v['verdict']}: {v['reason'][:50]}")

            # üî• FIX: Calculate pass/fail with proper score
            passed = actual_score >= float(threshold)
            if not passed: overall_passed = False
            
            # üî• FIX: Use DeepEval's reason if available, otherwise construct
            if reason is None:
                # Try to construct reason from verdicts
                irrelevant_reasons = [v['reason'] for v in verdicts if v['verdict'] == 'no']
                if irrelevant_reasons:
                    reason = f"Score is {actual_score:.2f} due to irrelevant statements: {'; '.join(irrelevant_reasons[:2])}"
                else:
                    reason = f"Score is {actual_score:.2f}"
            
            print(f"  üìä Score: {actual_score:.2f} / {threshold} ‚Üí {'‚úÖ PASS' if passed else '‚ùå FAIL'}")
            print(f"  üìù Reason: {reason[:100] if reason else 'N/A'}")
            
            # üî• NEW: Show metric error if present
            if hasattr(m, 'error') and m.error:
                print(f"  {Colors.FAIL}‚ùå Metric Error: {m.error}{Colors.ENDC}")

            eval_results.append({
                "metric_name": m.__class__.__name__,
                "score": actual_score,
                "reason": reason,
                "passed": passed,
                "threshold": threshold,
                "success": success,
                "error": getattr(m, 'error', None),
                "pipeline_flow": {
                    "judge_statements": list(statements) if statements else [],
                    "judge_verdicts": verdicts
                }
            })

        # 3. Final Build Package
        build_result = {
            "build_id": f"BUILD_{timestamp}",
            "timestamp": timestamp,
            "overall_status": "PASSED" if overall_passed else "FAILED",
            "pipeline": {
                "student_sut": sut_data,
                "teacher_judge": judge_data,
                "deepeval_analytics": eval_results
            },
            "environment": {
                "sut_hardware": sut_instance.last_metrics if hasattr(sut_instance, 'last_metrics') else {},
                "os": os.name
            }
        }

        # 4. Write to Disk
        file_path = os.path.join(abs_output_dir, f"build_result_{timestamp}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(build_result, f, indent=2, ensure_ascii=False)
        
        print(f"\n{Colors.HEADER}{Colors.BOLD}üèÜ BUILD RESULT: {file_path}{Colors.ENDC}")
        print(f"{Colors.HEADER}Status: {build_result['overall_status']}{Colors.ENDC}\n")
        
        return build_result

    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Build result generation failed: {e}{Colors.ENDC}")
        import traceback
        traceback.print_exc()
        return None