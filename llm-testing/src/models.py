import os
import requests
import json
import re
import google.generativeai as genai
from deepeval.models.base_model import DeepEvalBaseLLM
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List

# ANSI Colors for Terminal Output
class Colors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    WARNING = '\033[93m'
    FAIL = '\033[91m'
    ENDC = '\033[0m'
    BOLD = '\033[1m'
    DIM = '\033[2m'


class BaseReportingModel(DeepEvalBaseLLM):
    """Base class with common reporting functionality."""
    
    def __init__(self):
        self.last_prompt = ""
        self.last_response = ""
        self.last_metrics = {}
        self.generation_start_time = None
        self.generation_end_time = None
    
    def get_report_data(self) -> Dict[str, Any]:
        """Returns the current state of the model run for combined reporting."""
        return {
            "model": getattr(self, 'model_name', 'unknown'),
            "duration_seconds": (
                (self.generation_end_time - self.generation_start_time).total_seconds()
                if self.generation_start_time and self.generation_end_time else 0
            ),
            "prompt": self.last_prompt,
            "response": self.last_response,
            "metrics": self.last_metrics,
            "thought_process": getattr(self, 'last_thought_process', "")
        }

    def save_report(self, report_dir: str, prefix: str = "llm_analysis"):
        """Universal report saving method."""
        try:
            abs_report_dir = os.path.abspath(report_dir)
            Path(abs_report_dir).mkdir(parents=True, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            report_file = os.path.join(abs_report_dir, f"{prefix}_{timestamp}.json")
            markdown_file = os.path.join(abs_report_dir, f"{prefix}_{timestamp}.md")
            
            report_data = {
                "timestamp": timestamp,
                "model": getattr(self, 'model_name', 'unknown'),
                "endpoint": self._get_endpoint(),
                "generation_time": {
                    "start": self.generation_start_time.isoformat() if self.generation_start_time else None,
                    "end": self.generation_end_time.isoformat() if self.generation_end_time else None,
                    "duration_seconds": (
                        (self.generation_end_time - self.generation_start_time).total_seconds()
                        if self.generation_start_time and self.generation_end_time else None
                    )
                },
                "prompt": self.last_prompt,
                "response": self.last_response,
                "metrics": self.last_metrics
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            self._save_markdown_report(markdown_file, report_data)
            print(f"\n{Colors.OKGREEN}üìÑ Report saved: {report_file}{Colors.ENDC}")
            return report_file
            
        except Exception as e:
            print(f"{Colors.WARNING}‚ö†Ô∏è  Report save failed: {e}{Colors.ENDC}")
            return None
    
    def _get_endpoint(self) -> str:
        return "N/A"
    
    def _save_markdown_report(self, filepath: str, data: Dict[str, Any]):
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"# LLM Analysis Report\n\n")
            f.write(f"**Model:** {data['model']}\n")
            f.write(f"**Timestamp:** {data['timestamp']}\n")
            f.write(f"**Endpoint:** {data['endpoint']}\n\n")
            if data['generation_time']['duration_seconds']:
                f.write(f"## ‚è±Ô∏è Generation Time: {data['generation_time']['duration_seconds']:.2f}s\n\n")
            if data['metrics']:
                f.write(f"## üíª Hardware Metrics\n\n")
                self._write_metrics_markdown(f, data['metrics'])
            f.write(f"## üìù Prompt\n\n```\n{data['prompt']}\n```\n\n")
            f.write(f"## üìä Response\n\n{data['response']}\n\n")
            f.write(f"---\n*Generated by LLM Testing Framework*\n")
    
    def _write_metrics_markdown(self, f, metrics: Dict[str, Any]):
        if 'duration_ms' in metrics:
            f.write(f"- Duration: {metrics['duration_ms']/1000:.2f}s\n")
        if 'cpu' in metrics:
            cpu = metrics['cpu']
            f.write(f"- CPU Avg: {cpu.get('avg_usage_percent', 'N/A')}% | Peak: {cpu.get('peak_usage_percent', 'N/A')}%\n")
        if 'ram' in metrics:
            ram = metrics['ram']
            f.write(f"- RAM Avg: {ram.get('avg_used_gb', 'N/A')}GB | Peak: {ram.get('peak_used_gb', 'N/A')}GB\n")
        f.write("\n")


class OllamaModel(BaseReportingModel):
    def __init__(self, model_name: str, base_url: str):
        super().__init__()
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.api_key = os.getenv("SUT_API_KEY")

    def load_model(self): return self.model_name
    def _get_endpoint(self) -> str: return f"{self.base_url}/playwright"

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt
        self.generation_start_time = datetime.now()
        url = f"{self.base_url}/playwright"
        headers = {"x-api-key": self.api_key, "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"model": self.model_name, "messages": [{"role": "user", "content": prompt}], "stream": False}
        
        try:
            response = requests.post(url, json=payload, headers=headers, timeout=600)
            self.generation_end_time = datetime.now()
            if response.status_code != 200: return f"Ollama Error: {response.status_code}"
            res_json = response.json()
            if 'metrics' in res_json:
                self.last_metrics = res_json['metrics']
                self._display_metrics()
            content = self._extract_content(res_json)
            if content:
                self.last_response = content
                self.save_report("./data/results/ollama_reports", "ollama")
                return content
            return "No content in response"
        except Exception as e:
            self.generation_end_time = datetime.now()
            return f"Ollama Error: {str(e)}"
    
    def _extract_content(self, response: Dict[str, Any]) -> Optional[str]:
        if not isinstance(response, dict): return str(response)
        for key in ['analysis', 'response', 'content', 'text', 'output']:
            if key in response and response[key]: return response[key]
        if 'message' in response:
            msg = response['message']
            if isinstance(msg, dict) and 'content' in msg: return msg['content']
        return None
    
    def _display_metrics(self):
        if not self.last_metrics: return
        print(f"\n{Colors.BOLD}--- Hardware Metrics ---{Colors.ENDC}")
        if 'duration_ms' in self.last_metrics: print(f"‚è±Ô∏è  Duration: {self.last_metrics['duration_ms']/1000:.2f}s")
        print(f"{Colors.BOLD}------------------------{Colors.ENDC}\n")

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"Ollama: {self.model_name}"


class RobustGeminiModel(BaseReportingModel):
    def __init__(self, model_name: str):
        super().__init__()
        self.model_name = model_name
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        self.generation_config = {"temperature": 0.1, "top_p": 0.95, "top_k": 40}
        self.model = genai.GenerativeModel(model_name=model_name, generation_config=self.generation_config)

    def load_model(self): return self.model
    def _get_endpoint(self) -> str: return f"Gemini API: {self.model_name}"

    def _sanitize_json_response(self, text: str) -> str:
        text = re.sub(r'```json\s*', '', text)
        text = re.sub(r'```\s*', '', text)
        start = text.find('{'); end = text.rfind('}')
        if start != -1 and end != -1: text = text[start:end+1]
        return text.strip()

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt
        self.generation_start_time = datetime.now()
        enhanced_prompt = self._enhance_prompt_for_json(prompt)
        try:
            response = self.model.generate_content(enhanced_prompt)
            self.generation_end_time = datetime.now()
            sanitized_text = self._sanitize_json_response(response.text)
            self.last_response = sanitized_text
            self.save_report("./data/results/gemini_reports", "gemini")
            return self.last_response
        except Exception as e:
            self.generation_end_time = datetime.now()
            return f"Gemini Error: {str(e)}"
    
    def _enhance_prompt_for_json(self, prompt: str) -> str:
        if "json" in prompt.lower() or "{" in prompt:
            return f"{prompt}\n\nCRITICAL: Return ONLY valid JSON. No markdown."
        return prompt

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"Gemini ({self.model_name})"


class MultiAgentSUT(BaseReportingModel):
    def __init__(self, model_name: str, base_url: str, endpoint: str, api_key: str):
        super().__init__()
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.endpoint = endpoint
        self.api_key = api_key
        self.last_thought_process = ""

    def load_model(self): return self.base_url
    def _get_endpoint(self) -> str: return f"{self.base_url}{self.endpoint}"

    def generate(self, prompt: str) -> str:
        self.last_prompt = prompt; self.last_thought_process = ""
        self.generation_start_time = datetime.now()
        url = f"{self.base_url}{self.endpoint}"
        headers = {"x-api-key": self.api_key, "Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {"prompt": prompt, "model": self.model_name, "stream": True}
        full_analysis = ""
        try:
            response = requests.post(url, json=payload, headers=headers, stream=True, timeout=600)
            if response.status_code != 200: return f"MultiAgent Error: {response.status_code}"
            print(f"\n{Colors.OKCYAN}ü§ñ {self.model_name} generating...{Colors.ENDC}")
            for line in response.iter_lines():
                if line:
                    try:
                        decoded_line = line.decode('utf-8')
                        if decoded_line.startswith("data: "):
                            json_str = decoded_line.replace("data: ", "").strip()
                            if not json_str or json_str == "[DONE]": continue
                            data = json.loads(json_str)
                            if data.get('done'):
                                if 'metrics' in data: self.last_metrics = data['metrics']
                                break
                            dtype = data.get('type', 'analysis')
                            chunk = data.get('chunk') or data.get('content') or ''
                            if chunk:
                                if dtype == 'thought':
                                    self.last_thought_process += chunk
                                    print(f"{Colors.DIM}{chunk}{Colors.ENDC}", end="", flush=True)
                                else:
                                    full_analysis += chunk
                                    print(chunk, end="", flush=True)
                    except: continue
            self.generation_end_time = datetime.now(); self.last_response = full_analysis
            print(f"\n{Colors.OKGREEN}‚úÖ Complete{Colors.ENDC}\n")
            if self.last_metrics: self._display_metrics()
            self.save_report("./data/results/multiagent_reports", "multiagent")
            return full_analysis if full_analysis else "Error: Empty response"
        except Exception as e:
            self.generation_end_time = datetime.now(); return f"MultiAgent Error: {str(e)}"
    
    def _display_metrics(self):
        if not self.last_metrics: return
        print(f"{Colors.BOLD}--- Performance ---{Colors.ENDC}")
        if 'duration_ms' in self.last_metrics: print(f"‚è±Ô∏è  {self.last_metrics['duration_ms']/1000:.2f}s")
        print(f"{Colors.BOLD}-------------------{Colors.ENDC}\n")

    async def a_generate(self, prompt: str) -> str: return self.generate(prompt)
    def get_model_name(self): return f"MultiAgent: {self.model_name}"

# Compatibility Alias
GeminiModel = RobustGeminiModel

# ==========================================
# üöÄ BUILD RESULT REPORTER (NEW)
# ==========================================
def generate_build_result(sut_instance, judge_instance, metrics: List[Any], output_dir: str = "./data/results/builds"):
    """
    Combines SUT (Ollama/MultiAgent), Judge (Gemini), and DeepEval metrics 
    into a single 'Build Result'.
    """
    try:
        abs_output_dir = os.path.abspath(output_dir)
        Path(abs_output_dir).mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Collect SUT and Judge Data
        sut_data = sut_instance.get_report_data() if hasattr(sut_instance, 'get_report_data') else {}
        judge_data = judge_instance.get_report_data() if hasattr(judge_instance, 'get_report_data') else {}
        
        # Collect DeepEval Metrics
        eval_results = []
        overall_passed = True
        for m in metrics:
            eval_results.append({
                "metric": m.__class__.__name__,
                "score": getattr(m, 'score', 0),
                "reason": getattr(m, 'reason', "N/A"),
                "passed": getattr(m, 'success', False),
                "threshold": getattr(m, 'threshold', 0)
            })
            if not getattr(m, 'success', False):
                overall_passed = False

        build_result = {
            "build_id": f"BUILD_{timestamp}",
            "timestamp": timestamp,
            "overall_status": "PASSED" if overall_passed else "FAILED",
            "sut": sut_data,
            "judge": judge_data,
            "deepeval_metrics": eval_results
        }

        # Save Combined Build Result
        file_path = os.path.join(abs_output_dir, f"build_result_{timestamp}.json")
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(build_result, f, indent=2, ensure_ascii=False)
        
        print(f"\n{Colors.HEADER}{Colors.BOLD}üèÜ BUILD RESULT GENERATED: {file_path}{Colors.ENDC}")
        return build_result

    except Exception as e:
        print(f"{Colors.FAIL}‚ùå Failed to generate build result: {e}{Colors.ENDC}")
        return None